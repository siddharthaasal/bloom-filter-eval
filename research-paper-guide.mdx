# Empirical Evaluation of Bloom Filter Variants: Research Paper Guide

This document outlines the strategy, methodology, and narrative structure for your research paper, based on the benchmarking suite we have built.

---

## 1. Research Objective

**Title**: _Empirical Evaluation of Bloom Filter Variants Under Skewed, Adversarial, and Temporal Workloads_

**Core Question**: How do different probabilistic filter implementations—Standard, Counting, Scalable, and Cuckoo—behave under _realistic_ (non-uniform) and _adversarial_ conditions?

**Hypothesis**:

1.  **Skew (Zipfian)**: Filters with locality (like Cuckoo or Blocked BF) should benefit from skewed workloads due to CPU cache hits, whereas Standard BFs will show little improvement.
2.  **Adversarial**: Query performance will degrade significantly when attacking False Positives due to consistent collision resolution paths.
3.  **Temporal**: Long-running filters (Scalable/Counting) may fragment or degrade over time compared to fixed-capacity filters receiving batch rebuilds.

---

## 2. Methodology & Experimental Design

### A. The Contenders (Independent Variable 1)

| Filter Type               | Implementation Strategy      | Key Characteristic | Trade-off                                               |
| :------------------------ | :--------------------------- | :----------------- | :------------------------------------------------------ |
| **Standard Bloom Filter** | `pybloom_live` (Bitarray)    | Simple $k$ hashes. | No deletion, fixed size.                                |
| **Counting Bloom Filter** | **Custom** (8-bit counters)  | Array of counters. | Supports delete, $8\times$ memory usage.                |
| **Scalable Bloom Filter** | `pybloom_live` (Layered)     | Stack of SBFs.     | Grows indefinitely, slower query (checks all layers).   |
| **Cuckoo Filter**         | `cuckoofilter` (Fingerprint) | 2+ Hash buckets.   | Supports delete, high compactness, varying insert time. |

### B. The Workloads (Independent Variable 2)

We reject the "Uniform Random Only" standard of basic tutorials. Your paper focuses on **Realism**:

1.  **Uniform (Baseline)**: Control group. Random integers.
2.  **Zipfian (Skewed)**:
    - _Why_: Real traffic follows Power Laws (80/20 rule).
    - _Parameter_: $\alpha \in \{1.0, 1.2, 1.5\}$. Higher $\alpha$ = more skew (more "hot" keys).
    - _Expected Outcome_: Measuring if "hot" keys stay in L1/L2 cache, improving $P99$ latency.
3.  **Adversarial (Security)**:
    - _Why_: DDoS prevention often uses filters. Attackers mine False Positives to bypass filters.
    - _Strategy_: Mine keys that trigger `True` (False Positives) and replay them.
    - _Metric_: Does the filter panic? (Cuckoo might loop, Scalable might check all layers).
4.  **Temporal (Stability)**:
    - _Why_: Data isn't static. Hot sets shift over time.
    - _Strategy_: 3 Phases of shifting "Active Sets".
    - _Metric_: Efficiency over time.

### C. The Metrics (Dependent Variables)

1.  **Metric 1: False Positive Rate (FPR)**
    - _Goal_: Accuracy.
    - _Target_: Configured for $1\%$. Does it hold under skew? (Yes, usually, but Cuckoo can fill up).
2.  **Metric 2: Latency ($\mu$s)**
    - _Insert_: Cost of ingestion.
    - _Query (Avg)_: Throughput.
    - _Query (P99)_: **Crucial for Systems**. The "Tail Latency". Indicates cache misses or worst-case collision chains.
    - _Delete_: Maintenance cost (only for Counting/Cuckoo).
3.  **Metric 3: Memory Efficiency**
    - _Bits per Key_: Storage cost.
    - _Analysis_: Compare Custom Counting BF (heavy) vs Cuckoo (light).

---

## 3. Paper Structure (Proposed Outcome)

### I. Abstract

> "Probabilistic data structures are fundamental to modern systems... standard evaluations assume uniform distribution... this paper evaluates performance under realistic conditions..."

### II. Introduction

- Define the problem: Big Data requires approximate membership.
- The Gap: Most comparisons ignore that _access patterns are rarely uniform_.
- Your Contribution: A study of Skew, Adversarial, and Deletion costs.

### III. System Model (The Algorithms)

- Briefly explain the math of BF ($k$ hashes) vs Cuckoo (Fingerprints + partial relocations).
- **Implementation Note**: Explicitly mention using a _Custom Counting Bloom Filter_ with 8-bit counters (safe but expensive) vs standard 4-bit optimizations.

### IV. Evaluation (The "Meat")

_Use the CSV data in `src/results/` to generate these plots._

- **Exp 1: The Skew Effect**
  - _Graph_: X-axis = Zipf Alpha, Y-axis = P99 Latency.
  - _Analysis_: Does `StandardBloom` stay flat? Does `Cuckoo` get faster?
- **Exp 2: The Attack**
  - _Graph_: Bar chart of Query Latency (Uniform vs Adversarial).
  - _Analysis_: How much slower is querying a False Positive compared to a True Negative? (Usually slower because it checks _all_ bit positions/buckets).
- **Exp 3: Scalability Cost**
  - Compare `Standard` vs `Scalable`. Show the latency penalty paid for the ability to grow.

### V. Discussion & Guidelines

- **When to use Cuckoo**: High performance reads, need deletion, memory is tight.
- **When to use Counting**: Simplicity, heavy deletions, memory is abundant.
- **When to use Scalable**: Unknown dataset size (Zero-config).
- **Security**: Warning about algorithmic complexity attacks on filters.

### VI. Conclusion

- Summary of findings.
- "No one size fits all".

---

## 4. Technical Nuances to Highlight

1.  **The "Cache-Proxy" Metric**:
    - We cannot easily count L1 cache misses in Python.
    - _However_, `P99_QueryLatency` acts as a proxy. If P99 spikes, it effectively means we are fetching from main memory (RAM) instead of Cache.
    - Use this argument to discuss "Hardware-efficiency".
2.  **Implementation Bounds**:
    - Acknowledge that Python adds overhead. The _relative_ differences between filters matter more than absolute nanoseconds.
3.  **Adversarial Mining**:
    - Note that finding FPs becomes harder as $m/n$ increases. Secure systems should rotate hash seeds.

---

## 5. Next Steps for You

1.  **Generate Data**: Increase `NUM_KEYS` to 100k or 1M in `config.py` and run a full benchmark overnight.
2.  **Plotting**: Use `matplotlib` to verify the hypotheses above.
3.  **Drafting**: Start filling in Section III (System Model) while the benchmarks run.
